## TTS 모듈 구현 기록

여러 시도를 자주 해보는 편이지만, 막상 기록으로 남기지는 않는 경우가 많아 이번에는 간단한 구현이라도 남겨보기로 했다.  
개인적으로 만들고 있는 앱에 TTS 기능을 넣어보고 싶어 조금 만져본 내용을 정리한다. Pytorch로 직접 모델을 불러와 구현할 수도 있지만, 이번에는 Coqui TTs를 사용해보기로 했다.

---

### Coqui TTS
- 오픈소스 텍스트-음성 변환(Text-to-Speech, TTS) 프레임워크
- Tacotron2, FastSpeech, VITS 등 다양한 모델 구조 지원
- 여러 언어·데이터셋 기반의 학습 가능

먼저 기본 영어 모델을 사용해 간단히 테스트해봤다.

~~~
from TTS.api import TTS

# 기본 영어 모델로 테스트
tts = TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC")
tts.tts_to_file(text="Hello world. This is a test.", file_path="output.wav")
~~~

우리가 흔히 떠올리는 딱딱한 느낌의 영어 TTS 음성이 생성된다.
다음으로 다국어 모델인 XTTS-v2를 이용해 한국어 음성을 생성해보았다.

~~~
import os
import torch
from TTS.api import TTS

os.environ["COQUI_TOS_AGREED"] = "1"

device = "cuda" if torch.cuda.is_available() else "cpu"
tts = TTS(model_name="tts_models/multilingual/multi-dataset/xtts_v2").to(device)

# XTTS-v2 테스트
tts.tts_to_file(
    text="안녕하세요. 오늘은 날씨가 좋네요. 아이 캔 스피크 잉글리쉬.",
    language="ko",
    speaker_wav="output/output.wav",
    file_path="output/test_params.wav",
    
    # 제어 가능한 파라미터들
    temperature=0.65,           # 0.0~1.0, 낮을수록 안정적, 높을수록 다양함 (기본: 0.85)
    length_penalty=0.5,        # 길이 페널티 (기본: 1.0)
    repetition_penalty=7.0,    # 반복 방지 (기본: 2.0, 높을수록 반복 적음)
    top_k=50,                  # Top-K 샘플링 (기본: 50)
    top_p=0.85,                # Top-P 샘플링 (기본: 0.85)
    speed=2,                 # 속도 조절 (기본: 1.0)
    enable_text_splitting=False # 문장 분할 여부 (기본: True)
)
~~~

XTTS-v2는 짧은 샘플 음성을 기반으로 보이스 클로닝 기능을 제공한다.  
테스트를 진행하면서 다음과 같은 문제점을 확인했다.

**✔ 품질 관련 문제(어눌함, 억양 어색함 등)** 
- 샘플 음성이 길고 품질이 좋을수록 목소리 모사는 개선됨
- 그러나 늘어지는 발음, 어색한 억양은 여전히 존재
- 보이스 클로닝 품질의 한계로 보이며, 파인 튜닝을 고려할 필요가 있음

**✔ 텍스트 전처리 부재**  

언어를 지정해놓고 생성하더라도 다음과 같은 문제가 발생한다.
- 한글 문장 속 영어 단어를 자연스럽게 읽지 못함
- 맥락에 따라 다른 발음을 해야 하는 경우 제대로 처리하지 못함
    - Samsung → “삼성”
    - KT → “케이티”
    - 1950m → “천구백오십미터”
    - 학번/군번 → “일구 오공...” 식으로 읽어야 함

따라서 TTS 앞단에 텍스트 표준화/전처리 모듈이 필요하다고 판단했다.

---

### Step 1: Coqui TTS Fine-tuning 

제로샷 보이스 클로닝만으로는 한국어 TTS 품질이 만족스럽지 않았다. 다른 모델을 찾기보다는 일단 XTTS-v2를 파인튜닝해보기로 한다.

마침 XTTS-v2를 한국어로 파인 튜닝해보았다는 [블로그 글](https://bradjobs.notion.site/TTS-85a62e9706fe49a3876208f749ce8c35)이 있어 참고했다.

#### try #1 - epoch 10, LR 5e-6, 400개 샘플(37m)

- 어눌한 말투가 꽤 개선됨. 여러 번 생성하면 깔끔한 결과가 나오기 시작
- 된소리 발음 문제, 잡음 섞임 등 여전히 개선 여지 존재
- 데이터가 적었던 만큼, 샘플 수를 늘리는 게 필요하다고 판단

#### try #2 - epoch 30, LR 5e-6, 900개 샘플(67m)

- 학습 중 epoch 7부터 loss가 감소하지 않고 증가
- 데이터가 많아지면서 높은 LR로 인한 과적합이 빠르게 발생한 것으로 보임
- 최종 Eval Loss 2.71


#### try #3 - epoch 10, LR 1e-6, 900개 샘플(67m)

- LR 낮추고 epoch도 줄여 다시 실험
- epoch 10까지 지속적으로 Eval loss 감소. 최종 2.378까지 내려감
- 하지만 음성 품질은 여전히 기대치에는 미달. Loss 감소 추세를 보면 epoch을 조금 더 늘려볼 여지는 있음

#### try #4 - epoch 20, LR 1e-6, 900개 샘플(67m)

- 최종 2.28까지 내려감
- 그러나 실제 품질은 원하는 만큼 나오지 않음. 
- Loss는 계속해서 감소했던 것으로 보아 epoch을 더 높일 수도 있겠지만 크게 유의미하지는 않을 거라 판단
- 원하는 형태의 목소리에는 근접해가지만 떨림이나 잡음 등 불안정해지는 것을 확인

하루동안 시간을 썼으나 XTTS-v2로는 내가 원하는 수준에 이르기는 힘들다고 판단되어 다른 모델을 탐색하기로 결정

---

### Step 2: 다른 모델 탐색

XTTS-v2를 파인튜닝하면서 느낀 점은, 목표로 하는 음색에 점점 가까워지긴 하지만 생성 안정성이 떨어진다는 부분이었다.  
데이터 정제나 고품질 데이터셋 수집, 더 많은 학습 반복 등을 병행한다면 결과를 끌어올릴 여지는 있다. 그러나 "일단 한 번 해보자"라는 이번 실험의 취지에서는 다소 벗어난 방향이라 판단했다. 

그래서 HuggingFace에서 파인튜닝이 아니라 제로샷 보이스 클로닝을 지원하며 한국어 품질도 무난한 모델들을 먼저 테스트해보기로 했다.  
일단 적당한 품질의 음성이 나오는 모델을 찾은 뒤 → 텍스트 전처리기 개발 → 세부 품질 개선 순으로 단계를 밟는 편이 더 효율적이라고 생각했다

#### try #1 ChatterBox

ChatterBox는 최근 공개된 멀티모달 기반 TTS 모델로, 가볍게 사용할 수 있는 API 스타일 인터페이스와 제로샷 보이스 클로닝 기능을 강점으로 내세운 모델이다.  
구조적으로는 LLM 기반 오디오 인코더를 사용해 화자의 스타일·톤을 추출한 뒤, 이를 TTS 파이프라인에 반영하는 형태다.
특히 다국어 지원폭이 넓고, 클론 품질이 정제된 TTS 톤과 자연스러운 화자 톤 사이의 균형을 잘 맞춘다는 평이 있다.

- 제로샷 보이스 클로닝 지원 
    - 레퍼런스 음성을 audio_prompt_path로 넘기면 그 톤/목소리로 합성 가능
- 멀티링구얼 TTS
    - 총 23개 언어를 지원
    - 한국어 품질에 대해 “특별히 떨어진다”는 평은 찾지 못했음
- 실제 사용 소감
    - 기대 이상으로 안정적인 성능을 보여줌
    - 참조 음성과 “정돈된 TTS 음성”이 적당히 섞인 느낌의 결과물이 깔끔하게 생성됨
    - 목소리의 일관성과 자연스러움도 비교적 잘 유지됨

#### try #2 FishSpeech

#### try #3 GPT-SoVITS

---

### Step 3: 텍스트 정규화(Text Normalization)

텍스트 정규화는 모델이 처리하기 쉬운 형태로 문장을 표준화하는 과정이다. 유니코드 정리, 반복 문자 축소, 불필요한 기호 제거, 약어·이모티콘 치환, 숫자·날짜를 읽기 형태로 바꾸는 등의 작업이 포함된다. TTS 성능을 위해서 주로 Code-mixed 문제를 해결하는데 집중했다. 

**Code-mixed란?** 한 문장 안에 여러 언어가 섞여 있는 상태를 의미한다. 예를 들어 "오늘 meeting에서 API 스펙 의논하자" 처럼 한국어, 영어, 약어 등이 섞여 있는 형태로 영어의 경우, 'ko'를 지정하는 다국어 모델은 영어가 섞여 있는 부분을 제대로 발음하지 못 했다.

먼저 검색해서 가장 먼저 나오는 g2pk2 라이브러리를 사용해보았다.

~~~
from g2pk2 import G2p

g2p = G2p()

text = "오늘 meeting에서 API 스펙 의논하자"
print(g2p(text))
## 결과: 오늘 미팅에서 API 스펙 의논하자
~~~

바로 사용할 수 있는 라이브러리라는 것을 감안했을 때, 좋은 성능을 보였지만 문제점도 파악됐다.
- **약어 처리 불가:** 위에서 처럼 API 등의 약어는 처리 불가능했고, 'HD홀딩스 → H디홀딩스' 처럼 일부만 한글로 변형하는 경우가 있었다.
- **영어 발음 오류:** 'John → 잔', 'python → 파이산' 처럼 영어 발음을 잘못 출력하는 경우가 파악됐다.

그 외에도 여러 예외처리들이 필요하다는 것을 느끼고 직접 TTS 입력 전 전처리기 개발을 진행했다. 사실 하루이틀 정도의 시간이 소모될 줄 알았는데 깊게 팔수록 생각해야할 것들이 많아져 일주일정도 시간이 소요됐다. 

개발을 맨땅부터 진행하는 것은 나에겐 벅차서 일단 참고할만한 것들을 찾아보았고 [SMART-g2p](https://github.com/SMART-TTS/SMART-G2P)를 많이 참조했다. 다음으로 텍스트 정규화 개발 과정을 소개한다. 

텍스트 정규화 구조는 크게 다음과 같다. "텍스트 전처리 - 형태소 분석 - 각 형태소 변형 - 재결합" 으로 구성된다. 

- **텍스트 전처리**: 오탈자 및 형태소 분석 전에 처리해야할 예외처리 작업을 진행한다. 주로 정규식을 통한 처리가 진행된다.
- **형태소 분석**: mecab 라이브러리를 사용해서 문장의 형태소 분석을 진행한다.
- **형태소 변환**: 분해된 개별 형태소를 종류에 따라 처리한다. 한글은 그대로 내버려두고 숫자, 특수문자, 외래어들을 주로 처리한다. 
- **재결합**: 변형된 형태소들을 다시 재결합하여 문장 형태로 만든다.

#### Case #1: 숫자

숫자는 맥락에 따라 읽는 방식이 달라진다. 예를 들어 "3개"는 "세 개"로, "3번"은 "삼 번"으로 읽어야 한다. 이를 위해 형태소 분석 결과에서 숫자 뒤에 오는 단위를 확인하고, 해당 단위에 맞는 읽기 함수를 호출하도록 구현했다.

- **한자어 숫자** (`read_sino_kor`): 일반적인 숫자 읽기. 만, 억, 조 단위까지 지원하며, 십/백/천 단위에서 "일"을 생략하는 규칙 적용 (예: 10 → "십", 100 → "백")
- **고유어 숫자** (`read_native_kor`): 1~99까지 고유어로 읽기. "개", "명" 등의 단위와 함께 사용
- **단위별 읽기** (`read_counter_kor`): 뒤에 오는 단위에 따라 한자어/고유어를 선택. `lexicon.py`에 정의된 단위별 매핑 사용
- **자리수 읽기** (`read_only_num`): 소수점이나 버전 번호처럼 각 자리수를 개별적으로 읽는 경우 (예: 3.14 → "삼 점 일사")
- **영어 숫자** (`read_num_eng`): Code-mixed 상황에서 영어로 읽어야 하는 경우 (현재는 비활성화)

#### Case #2: 영어 및 약어

한국어 문장 속 영어 단어는 사전 기반 변환과 모델 기반 변환을 조합해 처리한다.

- **사전 기반 변환**: `base_eng2kor_dict.json`과 `user_eng2kor_dict.json`에 정의된 단어는 즉시 변환
- **약어 처리** (`read_acronym2kor`): 대문자만 있거나 모음이 없는 경우 약어로 판단해 각 알파벳을 개별적으로 읽음 (예: "API" → "에이피아이")
- **모델 기반 변환** (`read_engbymodel`): 사전에 없는 단어는 HuggingFace의 `eunsour/en-ko-transliterator` 모델을 사용해 한글 발음으로 변환. GPU 사용 가능 시 자동으로 GPU에 로드

#### Case #3: 특수문자 및 기호

특수문자는 앞뒤 맥락에 따라 한글로 읽거나 영어로 읽거나 무시한다.

- **한글 맥락**: 한글이 앞뒤에 있으면 한글 이름으로 읽기 (예: "@" → "골뱅이", "#" → "샵")
- **영어 맥락**: 영어가 앞뒤에 있으면 영어 이름으로 읽기 (예: "@" → "앳", "#" → "넘버")
- **화폐/단위 기호**: "$", "￦", "%" 등은 단위 기호로 인식해 한글로 변환 (예: "$" → "달러", "%" → "퍼센트")

#### 구현 세부사항

**1. 텍스트 전처리 단계** (`check_typos`, `correction_exception`)

형태소 분석 전에 정규식으로 처리할 수 있는 예외 케이스들을 먼저 처리한다.

- 오탈자 제거: 자모만 있는 경우, 비정상적인 공백 패턴 정규화
- 숫자 쉼표 제거: "3,200" → "3200" (천 단위 구분자 제거)
- 시간/날짜 변환: "09:10" → "구시 십분", "1996.6.15." → "천구백구십육년 육월 십오일"
- 영어 줄임말 보호: "don't" → "don_t" (형태소 분석 시 분리되지 않도록 임시 변환)

**2. 형태소 분석** (`align_text`)

Mecab을 사용해 문장을 형태소 단위로 분해한다. `konlpy.Mecab` 또는 `mecab_ko.Tagger`를 자동으로 선택하며, lazy initialization으로 필요할 때만 로드한다.

**3. 맥락 기반 변환** (`get_context`, `trans_bundle`)

각 형태소를 변환할 때 앞뒤 형태소 정보를 참조해 맥락에 맞는 변환을 수행한다.

- 숫자 변환: 뒤에 오는 단위(NNBC 품사)를 확인해 적절한 읽기 함수 선택
- 기호 변환: 앞뒤가 한글인지 영어인지 확인해 한글/영어 이름 선택
- 조사 교정: 앞 단어의 받침 유무에 따라 "은/는", "이/가" 등을 자동 교정

**4. 재결합 및 후처리**

변환된 형태소들을 공백으로 연결해 최종 문장을 생성한다. 문장 끝 구두점은 형태소 분석 전에 분리했다가 마지막에 다시 붙인다.

#### 사용 예시

~~~
from normalizer import trans_sentence

# 기본 사용
text = "오늘 meeting에서 API 스펙을 3개 의논하자"
result = trans_sentence(text)
print(result)
# 출력: 오늘 미팅에서 에이피아이 스펙을 세 개 의논하자
~~~

#### 한계 및 개선 방향

현재 구현에서도 여전히 해결되지 않은 케이스들이 있다.

- **맥락 의존적 발음**: 같은 단어라도 문맥에 따라 다르게 읽어야 하는 경우 (예: "read"의 과거형/현재형)
- **복합어 처리**: "iPhone"처럼 대소문자가 섞인 경우나 "C++" 같은 특수 케이스
- **도메인 특화 용어**: 전문 용어나 신조어는 사전에 수동으로 추가해야 함

향후 개선 방향으로 현재 모델을 byt5 혹은 NLLB-200을 파인튜닝한 모델로 변경하며 모델을 통한 외래어 변형 과정을 손볼 예정이다.
